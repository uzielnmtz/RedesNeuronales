{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal de tres capas\n",
    "\n",
    "Los números utilizados para ejercicio se pueden descargar de:\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import json, matplotlib\n",
    "s = json.load( open(\"styles/bmh_matplotlibrc.json\") )\n",
    "matplotlib.rcParams.update(s)\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(11, 5)\n",
    "colores = [\"#348ABD\", \"#A60628\",\"#06A628\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interact_manual, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos\n",
    "\n",
    "Los datos a utilizar son imágenes de dígitos.  En su formato original, se leen dos vectores:\n",
    "* Los datos de **entrada** vienen en un vector 3D.\n",
    "  * Cada renglón corresponde a un ejemplar de entrenamiento.\n",
    "  * En cada renglón hay una matriz 2D con las intensidades de los pixels.\n",
    "* Las etiquetas (dígito correcto que representan) vienen en un vector de una dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist.read import read, printFull\n",
    "\n",
    "filesDir = './'\n",
    "trainingSetFile = filesDir + 'train-images.idx3-ubyte'\n",
    "trainingSetLabelsFile = filesDir + 'train-labels.idx1-ubyte'\n",
    "testSetFile = filesDir + 't10k-images.idx3-ubyte'\n",
    "testSetLabelsFile = filesDir + 't10k-labels.idx1-ubyte'\n",
    "\n",
    "\n",
    "###     /\\ |__   __||  ____|| \\ | | / ____||_   _|/ __ \\ | \\ | |\n",
    "###    /  \\   | |   | |__   |  \\| || |       | | | |  | ||  \\| |\n",
    "###   / /\\ \\  | |   |  __|  | . ` || |       | | | |  | || . ` |\n",
    "###  / ____ \\ | |   | |____ | |\\  || |____  _| |_| |__| || |\\  |\n",
    "### /_/    \\_\\|_|   |______||_| \\_| \\_____||_____|\\____/ |_| \\_|\n",
    "### EN LAS SIQUIENTES DOS LINEAS SE LIMITA EL TAMAÑO DE LOS DATOS DE ENTRENAMIENTO,EN CASO\n",
    "### DE INCREMENTAR LA CANTIDAD EL CALCULO TOMA DRASTICAMENTE MAS TIEMPO EN EL CASO DE APROXIMACION DEL GRADIENTE,SE INCLUYE\n",
    "### IMPRESION DEL PROGRESO DEL METODO\n",
    "\n",
    "\n",
    "\n",
    "trainData = read(fileName=trainingSetFile).astype(np.float64)\n",
    "trainDataLabels = read(fileName=trainingSetLabelsFile).astype(np.float64)\n",
    "\n",
    "testData = read(fileName=testSetFile).astype(np.float64)\n",
    "testDataLabels = read(fileName=testSetLabelsFile).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist.plot\n",
    "from mnist.plot import muestraImagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    indice = (0, len(trainData) - 1)\n",
    ")\n",
    "def muestraImagenEntrenamiento(indice):\n",
    "    muestraImagen(trainData, trainDataLabels, indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder trabajar con la red neuronal, necesitaremos transformar esas entradas, de modo que los valores de las intensidades de los pixeles se encuentren en un solo renglón.  Las entradas a la red neuronal, deberán ser de la forma:\n",
    "\\begin{align}\n",
    "  X &= \\begin{bmatrix}\n",
    "       x_1^{(1)} ... x_n^{(1)}  \\\\\n",
    "       x_1^{(2)} ... x_n^{(2)}  \\\\\n",
    "       ...\\\\\n",
    "       x_1^{(m)} ... x_n^{(m)}\n",
    "      \\end{bmatrix}\n",
    "\\end{align}\n",
    "También necesitaremos que las etiquetas formen una matriz donde la única columna distinta de cero, sea la correspondiente al dígito correcto:\n",
    "\\begin{align}\n",
    "  Y &= \\begin{bmatrix}\n",
    "       0, ..., y_{label_0} = 1 , ... ,0 \\\\\n",
    "       ... \\\\\n",
    "       0, ..., y_{label_n} = 1 , ... ,0 \\\\\n",
    "      \\end{bmatrix}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define las matrices X y Y, como se muestra arriba\n",
    "## A partir de trainData y trainDataLabels\n",
    "## TIP: usar reshape\n",
    "def makeX(datosEntrenamiento):\n",
    "    pass\n",
    "\n",
    "def makeY(etiquetasEntrenamiento):\n",
    "    pass\n",
    "\n",
    "X = makeX(trainData)\n",
    "#print(\"X shape=\",X.shape)\n",
    "Y = makeY(trainDataLabels)\n",
    "#print(\"Y shape=\",Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repite lo mismo con los datos de entrenamiento\n",
    "XTest = makeX(testData)\n",
    "YTest = makeY(testDataLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red con tres capas\n",
    "La red neuronal que se utilizará es una red neuronal de tres capas:\n",
    "* Entrada\n",
    "* Oculta\n",
    "* Salida\n",
    "La forma genérica de la red se ilustra a continuación.  Sólo que la red de este ejercicio tendrá más neuronas en cada capa.\n",
    "<img src=\"figuras/Red3Capas.png\"/>\n",
    "Para este ejercicio el número de neuronas será:\n",
    "* 785 + 1 (28x28 pixeles más la unidad de sesgo)\n",
    "* 25 + 1 unidades en la capa oculta\n",
    "* 10 neuronas de salida (una por cada dígito)\n",
    "Por lo tanto, las dimensiones de las matrices de pesos son:\n",
    "* $\\Theta^{(0)} \\rightarrow (25 \\times 786)$\n",
    "* $\\Theta^{(1)} \\rightarrow (10 \\times 26)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta red tiene\n",
    "print(25*785 + 10*26, ' conexiones.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Programa una clase RedNeuronal con la arquitectura anterior.\n",
    "## Debe tener:\n",
    "## - como *atributos* las matrices de pesos Theta_0 y Theta_1\n",
    "## - un *constructor* que reciba como parámetro opcional otra red y copie sus pesos,\n",
    "##   si no recibe nada los inicializa aleatoriamente.\n",
    "## - método para asignar valores aleatorios a estas matrices\n",
    "## - método para devolver todos los pesos en una sola matriz columna.\n",
    "## - metodo para reconstruir matrices del tamaño de las matrices de pesos, a partir del vector.\n",
    "\n",
    "#from mnist.respuestas import RedMulticapa\n",
    "class RedMulticapa:\n",
    "    def generarAleatoriamente(n,m):\n",
    "        pass\n",
    "    \n",
    "    def __init__(self,otra=None):\n",
    "        pass\n",
    "    \n",
    "    def pesosAVector(self):\n",
    "        pass\n",
    "    \n",
    "    def reconstruirPesos(matriz):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Programa y verifica las función de activación logística g y su derivada.\n",
    "def logistica(val):\n",
    "    return 1/(1+np.exp(-val))\n",
    "\n",
    "def derLogistica(val):\n",
    "    return logistica(1-logistica(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Programa el método *feedforward(self, X, vector=None)*\n",
    "## Cuando reciba vector, usará estos pesos, en lugar de los propios.\n",
    "##\n",
    "def feedforward(self,X,vector=None):\n",
    "    pass\n",
    "\n",
    "setattr(RedMulticapa,'feedForward',feedforward)\n",
    "## Guarda, como atributos de la red, los resultados intermedios:\n",
    "## Zi, Ai\n",
    "##\n",
    "## Recuérda que puedes agregárselo a tu clase con\n",
    "## setattr(RedMulticapa, 'feedforward', feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mnist.plot)\n",
    "from mnist.plot import muestraActividad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Puede que aquí aparezca un Overflow... lo dejaremos pasar por el momento.\n",
    "RedA = RedMulticapa()\n",
    "RedA.feedForward(XTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist.plot import muestraActividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto sólo es ilustrativo, por eso usamos el conjunto de prueba,\n",
    "# que es más pequeño\n",
    "\n",
    "@interact(index = (0, len(XTest) - 1))\n",
    "def muestraActividad0(index):\n",
    "    muestraActividad(RedA, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encadenamiento hacia atrás\n",
    "En esta ocasión utilizaremos como función de error, a la entropía cruzada:\n",
    "\\begin{align}\n",
    "  J(\\Theta) =& - \\frac{1}{m} \\left[ \\sum_{i=0}^{m-1} \\sum_{k=1}^K   y_k^{(i)} \\log(h_\\Theta(x^{(i)}))_k  + (1 - y_k^{(i)}) \\log(1 - h_\\Theta(x^{(i)}))_k   \\right]\n",
    "\\end{align}\n",
    "\n",
    "Con esta función, el gradiente tiene la forma siguiente:\n",
    "* Para la última capa:\n",
    "\\begin{align}\n",
    "  \\delta^{(L)} &= (A^{(L)})^T - Y  \\\\\n",
    "\\end{align}\n",
    "* Para las capas anteriores:\n",
    "\\begin{align}\n",
    "  \\delta^{(l-1)} &= \\delta^{(l)} \\Theta_{[:,1:]}^{(l-1)} \\circ g'(z^{(l-1)}) \\\\\n",
    "\\end{align}\n",
    "* Para todas:\n",
    "\\begin{align}\n",
    "  \\Delta^{(l)} =& (\\delta^{(l+1)})^T A^{(l)}\\\\\n",
    "  \\nabla^{(l)} =& \\frac{1}{m}\\Delta^{(l)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Programa un método para calcular el error.\n",
    "## Esta vez utiliza la entropía cruzada.\n",
    "## Recuerda dar la opción de enviar un vector de pesos modificados.\n",
    "## Ignora de momento el parámetro lambdaR, sólo inclúyelo en la firma.\n",
    "import math\n",
    "def calcError(self,xtest,ytest,pesos,lambdaR):\n",
    "    pass\n",
    "    \n",
    "setattr(RedMulticapa,'calcError',calcError)\n",
    "\n",
    "RedB = RedMulticapa(RedA)\n",
    "RedB.calcError(XTest, YTest, RedB.pesosAVector(),0.0)#, lambdaR = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Programa el método *aproximaGradiente(self, X, Y, lambdaR = 0.0)*\n",
    "## Esta función perturbará los pesos uno a uno, por una cantidad\n",
    "## epsilon = 0.0004 y devolverá un vector con el gradiente.\n",
    "## Ignora de momento el parámetro lambdaR, sólo inclúyelo en la firma.\n",
    "def aproximaGradiente(self, X, Y):\n",
    "    pass\n",
    "\n",
    "setattr(RedMulticapa,'aproximaGradiente',aproximaGradiente)\n",
    "aprox = RedB.aproximaGradiente(X[0:1,:], YTest[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Programa el algoritmo de propagación hacia atrás\n",
    "## de modo que reciba X, Y y lambda=0.0\n",
    "## Guardará como atributos los valores calculados\n",
    "## para el error y los gradientes de las matrices de pesos\n",
    "\n",
    "def backPropagate(self,x,y,lambdaR=0.0):\n",
    "    pass\n",
    "    \n",
    "                             \n",
    "    \n",
    "setattr(RedMulticapa,'backPropagate',backPropagate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(index=(0,len(duo) - 1))\n",
    "def muestra_componente(index):\n",
    "    print(duo[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agrega ahora el método para realizar descenso por el gradiente\n",
    "#def descensoGradiente(red,gradiente):\n",
    "def descensoGradiente(self,X,Y,alfa,ciclos,lambdaR=0.0):\n",
    "    pass\n",
    "    \n",
    "setattr(RedMulticapa,'descensoGradiente',descensoGradiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tendrás que correr el entrenamiento varias veces para que el sistema funcione.\n",
    "# Toma en cuenta que X es una matriz bastante grande, por lo que será un poco tardado.\n",
    "# Observa que aparecerán advertencias de que el cálculo de la exponencial en la\n",
    "# función de evaluación está desbordando.\n",
    "@interact_manual()\n",
    "def entrenaRedC():\n",
    "    #red.descensoGradiente(X, Y, alfa=0.3, ciclos = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización\n",
    "Se buscará mantener los pesos pequeños para evitar los debordes y el sobreajuste a los datos\n",
    "de entrenamiento.  Para ello, la función $J$, además de medir el error en la predicción, será\n",
    "incrementada cuando los pesos incrementen su magnitud.\n",
    "\\begin{align}\n",
    "  J(\\Theta) =& - \\frac{1}{m} \\left[ \\sum_{i=1}^m \\sum_{k=1}^K   y_k^{(i)} \\log(h_\\Theta(x^{(i)}))_k  +\n",
    "            (1 - y_k^{(i)}) \\log(1 - h_\\Theta(x^{(i)}))_k   \\right]    + \\\\\n",
    "            & \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_L} \\sum_{j=1}^{s_{l+1}} (\\theta_{ji}^{(l)})^2\n",
    "\\end{align}\n",
    "De este modo, al calcular el gradiente se hace la siguiente modificación:\n",
    "\\begin{align}\n",
    " \\nabla^{(l)} =& \\frac{1}{m}\\Delta^{(l)} \\\\\n",
    " \\nabla^{(l)}[:,1:] =& \\nabla^{(l)}[:,1:] + \\frac{\\lambda}{m} \\Theta^{(l)}[:,1:]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados los pasos anteriores, sólo tienes que modificar ligeramente tu código\n",
    "# para incluir el parámetro lambda tanto en el cálculo del error,\n",
    "# como en el gradiente.\n",
    "# Cuando agregues el término de la regularización, recuerda hacer los cambios\n",
    "# únicamente sobre las componentes que no corresponden al bias.\n",
    "# Puedes utilizar notación como:\n",
    "# M[:,1:] = f(M[:,1:])\n",
    "# donde f(x) es una función cualquiera que depende de x.\n",
    "\n",
    "##RESUELTO EN LAS MISMAS FUNCIONES DE CALC ERROR Y BACKPROPAGATE CUANDO SE AGREGA UN PARAMETRO LAMBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el caso regularizado observa que las advertencias sobre el desbordamiento\n",
    "# en el cálculo de la exponencial desaparecen al continuar con el entrenamiento,\n",
    "# pues entra en acción el efecto de la regularización.\n",
    "# Puede suceder que por momentos aprezca de nuevos, pero su presencia ya no es permanente.\n",
    "@interact_manual()\n",
    "def entrenaRedC():\n",
    "    #red.descensoGradiente(X, Y, alfa=0.2, ciclos = 10, lambdaR = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Si ya funciona bien tu entrenamiento, corre algunos ciclos más para reducir el error\n",
    "# más significativament.\n",
    "@interact_manual()\n",
    "def entrenaRedC():\n",
    "    #red.descensoGradiente(X, Y, alfa=0.3, ciclos = 100, lambdaR = 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión\n",
    "Calcula los valores de la matriz de confusión para evaluar el desempeño de tu red después de haberla entrenado.\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr><th colspan=\"2\"></th><th colspan=\"2\">Clase predicha</th></tr>\n",
    "<tr><th colspan=\"2\"></th><th>1</th><th>0</th></tr>\n",
    "</thead>\n",
    "<tr><th rowspan=\"2\">Clase correcta</th><th>1</th><td>TP</td><td>FN</td></tr>\n",
    "<tr><th>0</th><td>FP</td><td>TN</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserta aquí el código para calcular la matriz de confusión\n",
    "def matrizDeConfusion(self,Y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibración\n",
    "\n",
    "Para elegir el valor correcto de $\\lambda$ deberás entrenar la red, para cada valor candidato, sobre el conjunto de *entrenamiento*.  Sin embargo, esto no bastará para quedarte con un $\\lambda$.  Toma en cuenta que, entre menor sea $\\lambda$, la hipótesis propuesta por la red puede ser más compleja y, por lo tanto, puede ajustarse mejor a los datos.  Eso no significa que vaya a ser adecuada para realizar predicciones.\n",
    "\n",
    "El siguiente paso es probar los sistemas entrenados ahora sobre los datos del conjunto de *validación*.  De este modo podrás evaluar, no sólo qué $\\lamda$ se ajusta mejor a los datos de entrenamiento, sino qué $\\lambda$ te da la mejor taza de predicción sobre datos no vistos previamente.\n",
    "\n",
    "Una vez elegido el modelo que da los mejores resultados en el conjunto de validación, puedes probarlo sobre el conjunto de *prueba*.  Esto te dará el mejor estimado de cómo se comportará el modelo sobre datos nuevos.\n",
    "\n",
    "Como punto extra puedes realizar esta face de calibrado.  Se te recomienda utilizar sólo un subconjunto de los datos disponibles, para que la ejecución termine más rápido.  Si quieres entrenar a tu sistema lo mejor posible, entonces tenle mucha paciencia a tu computadora y realiza las pruebas con los datos completos (tener más datos, en este escenario, hará que el ajuste se aproxime mejor a la función real que estamos buscando).  La mejor forma de hacerlo es automatizar las pruebas de valores candidatos, para que dejes a la computadora trabajando alguanas horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read() #or edit path to custom.css\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
